# -*- coding: utf-8 -*-
"""ADEC7430HW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SQJISzEKlu2O0-sQidsi1ceufaObsjkO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm
np.random.seed(42)

"""Generating Synthetic Data (10,000 rows; 2 Normal, 2 Uniform)

I generated a dataset with 10,000 rows. Two variables (x1, x2) are normally distributed with different means/variances, and two (x3, x4) are uniformly distributed. I added a squared term (x1_sq) and constructed the dependent variable y as a linear combination of all predictors, plus a small centered noise term (ε ~ N(0, 0.25)). This ensures the DGP is realistic but still analyzable with OLS.
"""

from numpy.random import default_rng
rng = default_rng(42)
n = 10_000

x1 = rng.normal(0, 1, n)
x2 = rng.normal(1.0, 2.0, n)

x3 = rng.uniform(-2.0, 2.0, n)
x4 = rng.uniform(0.0, 5.0, n)

x1_sq = x1**2
eps = rng.normal(0, 0.5, n)

beta_true = {"const": 1.25, "x1": 3.0, "x2": -2.0, "x3": 0.5, "x4": 1.5, "x1_sq": 0.8}

y = (beta_true["const"] + beta_true["x1"]*x1 + beta_true["x2"]*x2 + beta_true["x3"]*x3
     + beta_true["x4"]*x4 + beta_true["x1_sq"]*x1_sq + eps)

df = pd.DataFrame({"x1": x1, "x2": x2, "x3": x3, "x4": x4, "x1_sq": x1_sq, "y": y})
df.head()

"""Test/Train Split (70/30)

The dataset was split 70% training / 30% testing. This prevents overfitting and allows evaluation of generalization.
"""

features = ["x1", "x2", "x3", "x4", "x1_sq"]
X = df[features].copy()
y_all = df["y"].copy()

X_train, X_test, y_train, y_test = train_test_split(X, y_all, test_size=0.30, random_state=42)
X_train.shape, X_test.shape

"""Using statsmodels OLS, I estimated coefficients on the training set. The Train MSE ≈ 0.245 and Test MSE ≈ 0.252 are nearly identical, indicating that the model generalizes well. Residual diagnostics (scatter + histogram) confirm homoskedasticity and approximate normality, consistent with a correctly specified linear model."""

X_train_sm = sm.add_constant(X_train)
X_test_sm = sm.add_constant(X_test)

ols_model = sm.OLS(y_train, X_train_sm).fit()
yhat_train = ols_model.predict(X_train_sm)
yhat_test = ols_model.predict(X_test_sm)

mse_train = mean_squared_error(y_train, yhat_train)
mse_test = mean_squared_error(y_test, yhat_test)

print(ols_model.summary())
print("\nTrain MSE:", round(mse_train, 4))
print("Test  MSE:", round(mse_test, 4))

"""Residual Diagnostics (Train)"""

plt.figure()
plt.scatter(yhat_train, y_train - yhat_train, alpha=0.3)
plt.axhline(0, linestyle="--")
plt.xlabel("Fitted values (train)")
plt.ylabel("Residuals (train)")
plt.title("Residuals vs Fitted (Train)")
plt.show()

plt.figure()
plt.hist(y_train - yhat_train, bins=40)
plt.xlabel("Residual")
plt.ylabel("Frequency")
plt.title("Residuals Histogram (Train)")
plt.show()

"""Bootstrap Resamples

I resampled the dataset 10 times with replacement, re-estimated OLS each time, and recorded parameter estimates. This simulates sampling variability and provides an empirical distribution of coefficients.
"""

B = 10
coef_list = []
for b in range(B):
    idx = rng.integers(0, len(df), size=len(df))
    X_b = sm.add_constant(df.loc[idx, features])
    y_b = df.loc[idx, "y"].values
    model_b = sm.OLS(y_b, X_b).fit()
    coef_list.append(model_b.params)

boot_coef_df = pd.DataFrame(coef_list)
boot_coef_df.index.name = "bootstrap_id"
boot_coef_df

"""For each coefficient, I computed the bootstrap mean and standard deviation. Results show that bootstrap means are nearly identical to the OLS estimates, and the standard deviations are small given n=10,000 — confirming stability and precision."""

param_stats = pd.DataFrame({"mean": boot_coef_df.mean(axis=0),
                            "std": boot_coef_df.std(axis=0, ddof=1)})
param_stats

ols_params = ols_model.params.rename("train_OLS_coef")
comp = pd.concat([ols_params, param_stats], axis=1)
comp.round(4)

"""With a correctly specified linear model (including the x1^2 term) and a large n=10,000, the OLS coefficients are estimated very precisely. The bootstrap means are close to the train OLS point estimates, and the bootstrap standard deviations are small, indicating low sampling variability.
Train and test MSE are very similar, consistent with low overfitting. The OLS coefficients from step 4 fall within the tight range implied by the bootstrap means ± 2·std. This demonstrates consistency and robustness of OLS estimates. Train and test errors, plus diagnostics, support that the model fits the DGP well.

The residual plots confirm that there are no major violations of any OLS assumptions, and the close agreement between the OLS and bootstrap estimates highlights the reliability of inference in large samples. If the squared term(x1_sq) were omitted, both the residual plots and test error would have shown systematic bias, emphasizing the importance of proper model specification.

"""